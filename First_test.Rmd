---
title: "First Test"
output:
  html_document:
    df_print: paged
---

```{r}
library(MASS)
library(norm)
library(VIM)
library(ggplot2)
library(ggpubr)
library(naniar)
library(usethis)
library(MissMech) # removed from the CRAN repository.
#library(BaylorEdPsych) # removed from the CRAN repository.
#library(mvnmle) # cannot find 
library(rlist)
library(boot)
library(devtools)
library(Amelia)
library(abind)
library(missMethods)
library(gbutils)
library(pracma)
library(gdata)
#library(caret)
library(stats)
library(missForest)
library(missMDA)
library(uwo4419)
library(qualvar)
source_url('https://raw.githubusercontent.com/R-miss-tastic/website/master/static/how-to/generate/amputation.R')
source_url('https://raw.githubusercontent.com/njtierney/naniar/master/R/mcar-test.R')
source_url('https://raw.githubusercontent.com/njtierney/naniar/master/R/utils.R')
source_url('https://raw.githubusercontent.com/LqNoob/Machine-Learning-Evaluation-Metrics/master/R/Classification.R')



source("functions.R")
set.seed(43)
```


### 1.Generate data
#### (1)Complete data
> We generate a complete data set (Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8) with (Y1, Y2, Y3) ~ N(u1, S1), (Y4, Y5)~N(u2,S2), Y6~P(\lambda) and Y7,Y8~Binomial(prob). 


```{r}
n = 10000
mu.X = c(1, 2, 3)
Sigma.X = matrix(c(9, 3, 2,
                    3, 4, 0,
                    2, 0, 1), nrow = 3)
X.complete.cont = mvrnorm(n, mu.X, Sigma.X)#multivariate normal distribution

mu1.X = c(9, 8)
Sigma1.X = matrix(c(16, 14,
                     14, 25), nrow = 2)

X.complete.cont1 = mvrnorm(n, mu1.X, Sigma1.X)#multivariate normal distribution

lambda = 4.3
X.complete.discr = rpois(n, lambda) #poisson distribution


X.complete.cat = rbinom(n, size=5, prob = 0.4) #binomial

X.complete.cat2 = rbinom(n, size=7, prob = 0.6) #binomial

X.complete = data.frame(cbind(X.complete.cont,X.complete.cont1, X.complete.discr, X.complete.cat, X.complete.cat2))
X.complete[,7] = as.factor(X.complete[,7])
levels(X.complete[,7]) = c("F", "E", "D", "C", "B", "A")
X.complete[,8] = as.factor(X.complete[,8])
colnames(X.complete) = c("Y1", "Y2", "Y3", "Y4", "Y5", "Y6", "Y7", "Y8")
head(X.complete)
```



#### (2)Add missingness

```{r}
#rs = generate_miss(X.complete, 0.5, mechanism = "MNAR2")
```

###### (2.1)Continous variables with missing data
>We generate missing values first on the continuous variables (V1,...,V5)

```{r}
rs.con = generate_miss_ls(X.complete[,1:5],0.4)
```

###### (2.2)Continous variables + discrete varaibles with missing data
>We generate missing values on the variables (V1,...,V6)

```{r}
rs.con_dis = generate_miss_ls(X.complete[,1:6],0.4)
```

###### (2.3)Continous variables + discrete varaibles + categorical variables with missing data
```{r}
rs.mix = generate_miss_ls(X.complete,0.4)
```


### 2.Preprocessing
```{r}
list_df.con = list(rs.con$mcar$X.incomp,rs.con$mar1$X.incomp,rs.con$mar2$X.incomp,rs.con$mar3$X.incomp,rs.con$mnar1$X.incomp,rs.con$mnar2$X.incomp)

list_df.con_dis = list(rs.con_dis$mcar$X.incomp,rs.con_dis$mar1$X.incomp,rs.con_dis$mar2$X.incomp,rs.con_dis$mar3$X.incomp,rs.con_dis$mnar1$X.incomp,rs.con_dis$mnar2$X.incomp)

list_df.mix = list(rs.mix$mcar$X.incomp,rs.mix$mar1$X.incomp,rs.mix$mar2$X.incomp,rs.mix$mar3$X.incomp,rs.mix$mnar1$X.incomp,rs.mix$mnar2$X.incomp)
```


```{r}
#Preprocess for X.complete 
idx_col_num = c(1:6)
idx_col_cat = c(7:8)
X.complete.ord = ordinal_encode(X.complete, idx_col_cat)
#X.complete.norm = normalize_num(X.complete.ord, idx_col_num)

# Ordinal encoder for list_df.mix
idx_col_num = c(1:6)
idx_col_cat = c(7:8)
i=1
for(df in list_df.mix){
   list_df.mix[[i]] = ordinal_encode(list_df.mix[[i]], idx_col_cat)
    i = i + 1
}

```


### 3.Statistical test for MCAR 
```{r}
p_val = 0.05
```

#### (1)Test with dummy variables

```{r}
#dummy_test(rs.con$X.mcar,c())
test_result_dum = data.frame()#A dataframe that record th p-values for each test
ls_test = list("MCAR","MAR1","MAR2","MAR3","MNAR1","MNAR2")
i = 1
col_cat = c(7:8)
for(ls in list(list_df.con,list_df.con_dis,list_df.mix)){
  j = 1
  for(df in ls){
    if(i==3){
      out = dummy_test(df,col_cat)
    }
    else{
      out = dummy_test(df,c())
    }
    test_result_dum[i,ls_test[[j]]] = out$p.value
    j = j+1
  }
  i = i+1
}
row.names(test_result_dum) = c("Continuous", "Continuous+Discret","Continous+Discret+Categorical")
print(test_result_dum)

final_result_mcar = data.frame(test_result_dum>p_val)
final_result_mcar
```

#### (2)Test with Little's MCAR test

```{r}
test_result_little = data.frame()#A dataframe that record th p-values for each test
ls_test = list("MCAR","MAR1","MAR2","MAR3","MNAR1","MNAR2")
i = 1
for(ls in list(list_df.con,list_df.con_dis,list_df.mix)){
  j = 1
  for(df in ls){
    out = mcar_test(df)
    test_result_little[i,ls_test[[j]]] = out$p.value
    j = j+1
  }
  i = i+1
}
row.names(test_result_little) = c("Continuous", "Continuous+Discret","Continous+Discret+Categorical")
print(test_result_little)

final_result_mcar = data.frame(test_result_little>p_val)
final_result_mcar

```

#### (3)Test with Hawkin's test and nonparametric test
>It is slower than mcar_test.
I find the result is less reliable than the mcar_test, and it couldn't deal with large number of data

```{r}
test_result_hawkins = data.frame()#A dataframe that record th p-values for each Hawkin's test
test_result_nonparam = data.frame()#A dataframe that record th p-values for each nonparametric test
ls_test = list("MCAR","MAR1","MAR2","MAR3","MNAR1","MNAR2")
i=1
for(ls in list(list_df.con,list_df.con_dis,list_df.mix)){
  j=1
  for(df in ls){
    out_missmech = TestMCARNormality(df)
    test_result_hawkins[i,ls_test[[j]]]=out_missmech$pvalcomb
    test_result_nonparam[i,ls_test[[j]]]=out_missmech$pnormality
    j=j+1
  }
  i=i+1
}

row.names(test_result_hawkins) = c("Continuous", "Continuous+Discret","Continous+Discret+Categorical")
row.names(test_result_nonparam) = c("Continuous", "Continuous+Discret","Continous+Discret+Categorical")

test_result_hawkins
test_result_nonparam

final_result_mcar = data.frame(test_result_hawkins>p_val)*data.frame(test_result_nonparam<p_val)+data.frame(test_result_hawkins<p_val)
final_result_mcar = data.frame(final_result_mcar==0)
final_result_mcar
```



### 4.Evaluation matrix
```{r}
#dens_comp
#ls_MSE
#ls_F1
```

### 5.Create several datasets

```{r}
# Create bootstrap and jackknife datasets
# Bootstrap needs big number of samples
# Jackknife has a problem with categorical variables
df_with_mv = list_df.mix[[2]]
# names_cat = names(dict_name_cat)
# for(name in names_cat){
#   df_with_mv[[name]]=factor(df_with_mv[[name]])
#   levels(df_with_mv[[name]])=dict_name_cat[[name]]
# }
ls_boot = bootsample(df_with_mv,4)
ls_jack = jacksample(df_with_mv,4)
```

```{r}
#Create a dictionary {Y7:Y7_1,Y7_2,...}, {Y_7: Y_7_1, Y_7_2,...}
dict_onehot = function(df, col_cat){
  dict_name = list()
  ls_col_name = colnames(df)
  for(i in col_cat){
    col_cat_name = ls_col_name[i]
    dict_name[[col_cat_name]]=paste(col_cat_name, levels(df[[col_cat_name]]), sep = "_")
  }
  return(dict_name)
}

dict_name_cat = dict_onehot(df_with_mv, c(7:8))
```



```{r}
#Imputation (for example PCA with 3 dimension)
ls.imp.pca.onehot = list()
ls.imp.pca.fact = list()
idx_col_cat = c(7:8)
i = 1
num_col = ncol(df_with_mv)
for(df in ls_boot){
  df = factor_encode(df, idx_col_cat)
  pca = imputeFAMD(df, ncp = 3, maxiter = 20)
  ls.imp.pca.onehot[[i]] = data.frame(pca$tab.disj)
  ls.imp.pca.fact[[i]] = pca$completeObs
  i = i + 1
}
```

```{r}
rs0 = combine_boot(ls.imp.pca.fact,col_con=c(1:5),col_dis=c(6),col_cat=c(7:8), num_row_origin = 10000, method="factor",dict_cat=dict_name_cat, var_cat = 'wilcox_va')
rs = combine_boot(ls.imp.pca.onehot,col_con=c(1:5),col_dis=c(6),col_cat=c(7:20), num_row_origin = 10000, method="onehot", dict_cat=dict_name_cat, var_cat = 'unalike')
```




```{r}
#Imputation for ls_jack (for example PCA with 3 dimension)
ls.imp.pca.onehot = list()
ls.imp.pca.fact = list()
i = 1
num_col = ncol(df_with_mv)
for(df in ls_jack){
  df = factor_encode(df, idx_col_cat)
  #Input: an incomplete dataset
  #Outout: an imputed dataset (onehot or factor for categorical)
  pca = imputeFAMD(df, ncp = 3, maxiter = 20)
  ls.imp.pca.onehot[[i]] = data.frame(pca$tab.disj)
  ls.imp.pca.fact[[i]] = pca$completeObs
  i = i + 1
}

```

```{r}
df_full = df_with_mv
df_full = factor_encode(df_with_mv, idx_col_cat)
pca_full = imputeFAMD(df_full, ncp = 3, maxiter = 20)
imp.pca_full.onehot = pca_full$tab.disj
imp.pca_full.fact = pca_full$completeObs
```

```{r}
rs_jack0=combine_jack(ls.imp.pca.onehot,data.frame(imp.pca_full.onehot),col_con=c(1:5),col_dis=c(6),col_cat=c(7:20), method="onehot", dict_cat=dict_name_cat,var_cat = 'unalike')
rs_jack1=combine_jack(ls.imp.pca.onehot,data.frame(imp.pca_full.onehot),col_con=c(1:5),col_dis=c(6),col_cat=c(7:20), method="onehot", dict_cat=dict_name_cat,var_cat = 'wilcox_va')
```





### 6.Imputation

```{r}
#list_df.con = list(X.mcar.con,X.mar1.con,X.mar2.con,X.mnar1.con,X.mnar2.con)
#list_df.con_dis = list(X.mcar.con_dis,X.mar1.con_dis,X.mar2.con_dis,X.mnar1.con_dis,X.mnar2.con_dis)
#list_df.mix = list(X.mcar.mix,X.mar1.mix,X.mar2.mix,X.mnar1.mix,X.mnar2.mix)
```

#### (1)EM
```{r}

```


#### (2)EM+Importance resampling
```{r}
#EMB
#multivariate normal assumption
df_comp = X.complete.ord
df_with_mv = list_df.mix[[2]]
mask = rs.mix$mar1$R.mask
col_num = c(1:6)
col_cat = c(7:8)
num_imp = 5 #number of imputations

#imputation
imp_amelia = amelia(df_with_mv, m=num_imp, p2s=0, ords=col_cat)
all.matrix = abind(imp_amelia$imputations, along=3)
# average the imputation results
result_imp_em_mi = data.frame(apply(all.matrix, c(1,2), mean))
# for categorical columns, we round the category to the nearest one (in the situation of ordinal encoding)
result_imp_em_mi[,col_cat] = round(result_imp_em_mi[,col_cat])
variance_imp_em_mi = data.frame(apply(all.matrix, c(1,2), var))

#performance
list_df.mix[[2]]
result_imp_em_mi
variance_imp_em_mi
dens_comp(df_comp[,col_num],result_imp_em_mi[,col_num])
ls_MSE(df_comp,imp_amelia$imputations,mask,col_num)
ls_F1(df_comp,imp_amelia$imputations,mask,col_cat)
```


#### (3)Missforest


```{r}
list_df.mix[[2]] = factor_encode(list_df.mix[[2]], idx_col_cat)
imp.forest = missForest(xmis = list_df.mix[[2]], maxiter = 5, ntree = 100, dict_cat = dict_name_cat)
imp.forest$ximp
imp.forest$ximp.disj
```

```{r}
ls_MSE(df_comp,list(imp.forest$ximp),mask,col_num)
ls_F1(df_comp,list(imp.forest$ximp),mask,col_cat)
```


#### (4)MissMDA

```{r}
list_df.mix[[2]] = factor_encode(list_df.mix[[2]], idx_col_cat)
ncp.pca = estim_ncpFAMD(list_df.mix[[2]], method.cv="Kfold", verbose=T, maxiter = 50)$ncp
pca = imputeFAMD(list_df.mix[[2]], ncp = 5)
imp.pca = pca$comp
```

```{r}
imp.pca = ordinal_encode(imp.pca, idx_col_cat)
imp.pca
ls_MSE(df_comp,list(imp.pca),mask,col_num)
ls_F1(df_comp,list(imp.pca),mask,col_cat)
```


### 4.Imputation with bootstrapped dataset
```{r}
#Bootstrap with package, need to pass the estimated funstion inside
#bootdata = boot(data = ,funcs, R=500)

#

```






















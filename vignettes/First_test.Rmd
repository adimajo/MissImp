---
title: "First Test"
output:
  rmarkdown::html_vignette
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
---

```{r dependencies, message=FALSE, warning=FALSE}
library(MASS)
library(norm)
library(VIM)
# library(ggplot2)
# library(ggpubr)
library(naniar)
library(usethis)
# library(MissMech) # removed from the CRAN repository.
# library(BaylorEdPsych) # removed from the CRAN repository.
# library(mvnmle) # cannot find
library(rlist)
library(boot)
library(devtools)
library(Amelia)
library(abind)
library(missMethods)
library(gbutils)
library(pracma)
library(gdata)
library(caret)
library(stats)
library(missMDA)
library(uwo4419)
library(qualvar)
library(dplyr)
library(data.table)
library(mix)
library(ranger)
library(FactoMineR)
# source_url('https://raw.githubusercontent.com/R-miss-tastic/website/master/static/how-to/generate/amputation.R')
# source_url('https://raw.githubusercontent.com/njtierney/naniar/master/R/mcar-test.R')
# source_url('https://raw.githubusercontent.com/njtierney/naniar/master/R/utils.R')
# source_url('https://raw.githubusercontent.com/LqNoob/Machine-Learning-Evaluation-Metrics/master/R/Classification.R')



library(MissImp)
# source("../R/utils.R")
# source("../R/generate_missingness.R")
# source("../R/produce_NA.R")
# source("../R/dummy_test_MCAR.R")
# source("../R/MissMech_TestNormality.R")
# source("../R/miss_ranger_function.R")
# source("../R/bootstrap.R")
# source("../R/jackknife.R")
# source("../R/missMDA_function.R")
# source("../R/mcar_test_combined.R")
# source("../R/VIM_kNN.R")

set.seed(43)
```


### Generate data
#### Complete data
We generate a complete data set (Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8) with (Y1, Y2, Y3) ~ N(u1, S1), (Y4, Y5)~N(u2,S2), Y6~P(\lambda) and Y7,Y8~Binomial(prob). 


```{r Generate_complete_dataframe}
n <- 10000
mu.X <- c(1, 2, 3)
Sigma.X <- matrix(c(
  9, 3, 2,
  3, 4, 0,
  2, 0, 1
), nrow = 3)
X.complete.cont <- MASS::mvrnorm(n, mu.X, Sigma.X) # multivariate normal distribution

mu1.X <- c(9, 8)
Sigma1.X <- matrix(c(
  16, 14,
  14, 25
), nrow = 2)

X.complete.cont1 <- MASS::mvrnorm(n, mu1.X, Sigma1.X) # multivariate normal distribution

lambda <- 4.3
X.complete.discr <- stats::rpois(n, lambda) # poisson distribution


X.complete.cat <- stats::rbinom(n, size = 5, prob = 0.4) # binomial

X.complete.cat2 <- stats::rbinom(n, size = 7, prob = 0.6) # binomial

X.complete <- data.frame(cbind(X.complete.cont, X.complete.cont1, X.complete.discr, X.complete.cat, X.complete.cat2))
X.complete[, 7] <- as.factor(X.complete[, 7])
levels(X.complete[, 7]) <- c("F", "E", "D", "C", "B", "A")
X.complete[, 8] <- as.factor(X.complete[, 8])
colnames(X.complete) <- c("Y1", "Y2", "Y3", "Y4", "Y5", "Y6", "Y7", "Y8")
head(X.complete)
```



#### Add missingness

```{r add_miss_one_mechanism}
# rs = generate_miss(X.complete, 0.5, mechanism = "MNAR2")
```

We generate missing values first on the continuous variables (V1,...,V5), with the result in rs.con; 
on the continous + discrete varaibles (V1,...,V6), with the result in rs.con_dis; and on continous + discrete + categorical variables, with the result in rs.mix.

```{r add_miss_all_mechanism}
rs.con <- generate_miss_ls(X.complete[, 1:5], 0.4)
rs.con_dis <- generate_miss_ls(X.complete[, 1:6], 0.4)
rs.mix <- generate_miss_ls(X.complete, 0.4)
```




### Preprocessing
```{r list_of_incomplete_dataframes}
list_df.con <- list(rs.con$mcar$X.incomp, rs.con$mar1$X.incomp, rs.con$mar2$X.incomp, rs.con$mar3$X.incomp, rs.con$mnar1$X.incomp, rs.con$mnar2$X.incomp)

list_df.con_dis <- list(rs.con_dis$mcar$X.incomp, rs.con_dis$mar1$X.incomp, rs.con_dis$mar2$X.incomp, rs.con_dis$mar3$X.incomp, rs.con_dis$mnar1$X.incomp, rs.con_dis$mnar2$X.incomp)

list_df.mix <- list(rs.mix$mcar$X.incomp, rs.mix$mar1$X.incomp, rs.mix$mar2$X.incomp, rs.mix$mar3$X.incomp, rs.mix$mnar1$X.incomp, rs.mix$mnar2$X.incomp)
```


```{r preprocessing}
# Preprocess for X.complete
idx_col_num <- c(1:6)
idx_col_cat <- c(7:8)
X.complete.ord <- ordinal_encode(X.complete, idx_col_cat)
# X.complete.norm = normalize_num(X.complete.ord, idx_col_num)

# Ordinal encoder for list_df.mix
idx_col_num <- c(1:6)
idx_col_cat <- c(7:8)
i <- 1
for (df in list_df.mix) {
  list_df.mix[[i]] <- ordinal_encode(list_df.mix[[i]], idx_col_cat)
  i <- i + 1
}
```


### Statistical test for MCAR 
```{r p_val}
p_val <- 0.05
mcar_test_combined(list_df.mix[[3]], col_cat = c(7:8), p_val = p_val)$test_results
```

#### Test with dummy variables

```{r dummy_test}
# dummy_test(rs.con$X.mcar,c())
test_result_dum <- data.frame() # A dataframe that record th p-values for each test
ls_test <- list("MCAR", "MAR1", "MAR2", "MAR3", "MNAR1", "MNAR2")
i <- 1
col_cat <- c(7:8)
for (ls in list(list_df.con, list_df.con_dis, list_df.mix)) {
  j <- 1
  for (df in ls) {
    if (i == 3) {
      out <- dummy_test(df, col_cat)
    }
    else {
      out <- dummy_test(df, c())
    }
    test_result_dum[i, ls_test[[j]]] <- out$p.value
    j <- j + 1
  }
  i <- i + 1
}
row.names(test_result_dum) <- c("Continuous", "Continuous+Discret", "Continous+Discret+Categorical")
print(test_result_dum)

final_result_mcar <- data.frame(test_result_dum > p_val)
final_result_mcar
```




#### Test with Little's MCAR test

```{r little's_MCAR_test}
test_result_little <- data.frame() # A dataframe that record th p-values for each test
ls_test <- list("MCAR", "MAR1", "MAR2", "MAR3", "MNAR1", "MNAR2")
i <- 1
for (ls in list(list_df.con, list_df.con_dis, list_df.mix)) {
  j <- 1
  for (df in ls) {
    out <- mcar_test(df)
    test_result_little[i, ls_test[[j]]] <- out$p.value
    j <- j + 1
  }
  i <- i + 1
}
row.names(test_result_little) <- c("Continuous", "Continuous+Discret", "Continous+Discret+Categorical")
print(test_result_little)

final_result_mcar <- data.frame(test_result_little > p_val)
final_result_mcar
```

#### Test with Hawkin's test and nonparametric test
It is slower than mcar_test.
I find the result is less reliable than the mcar_test, and it couldn't deal with large number of data

```{r missMech_test}
test_result_hawkins <- data.frame() # A dataframe that record th p-values for each Hawkin's test
test_result_nonparam <- data.frame() # A dataframe that record th p-values for each nonparametric test
ls_test <- list("MCAR", "MAR1", "MAR2", "MAR3", "MNAR1", "MNAR2")
i <- 1
for (ls in list(list_df.con, list_df.con_dis, list_df.mix)) {
  j <- 1
  for (df in ls) {
    out_missmech <- TestMCARNormality(df)
    test_result_hawkins[i, ls_test[[j]]] <- out_missmech$pvalcomb
    test_result_nonparam[i, ls_test[[j]]] <- out_missmech$pnormality
    j <- j + 1
  }
  i <- i + 1
}

row.names(test_result_hawkins) <- c("Continuous", "Continuous+Discret", "Continous+Discret+Categorical")
row.names(test_result_nonparam) <- c("Continuous", "Continuous+Discret", "Continous+Discret+Categorical")

test_result_hawkins
test_result_nonparam

final_result_mcar <- data.frame(test_result_hawkins < p_val) * data.frame(test_result_nonparam < p_val)
final_result_mcar <- data.frame(final_result_mcar == 0)
final_result_mcar
```




### Evaluation matrix
```{r}
# dens_comp
# ls_MSE
# ls_F1
```

### Create several datasets

```{r}
# source("../R/single_imp_combined.R")
df <- list_df.mix[[2]]
col_cat <- c(7:8)
col_dis <- c(6)
n_resample <- 4 # 2*log(nrow(df))
maxiter_tree <- 3
maxiter_pca <- 30
ncp_pca <- ncol(df) / 2
learn_ncp <- FALSE

imp_method <- "PCA"
resample_method <- "jackknife"
cat_combine_by <- "onehot"
var_cat <- "unalike"
```


```{r}
res <- single_imp(
  df = df, imp_method = imp_method, resample_method = resample_method,
  n_resample = n_resample, col_cat = col_cat, col_dis = col_dis,
  maxiter_tree = maxiter_tree, maxiter_pca = maxiter_pca, ncp_pca = ncp_pca,
  learn_ncp = learn_ncp, cat_combine_by = cat_combine_by, var_cat = var_cat
)
```



```{r}
head(df)
head(res$imp)
head(res$imp.disj)
head(res$uncertainty)
head(res$uncertainty.disj)
```






```{r bootstrap_and_jackknife}
# Create bootstrap and jackknife datasets
# Bootstrap needs big number of samples
# Jackknife has a problem with categorical variables
df_with_mv <- list_df.mix[[2]]
df_with_mv <- factor_encode(df_with_mv, c(7:8))
ls_boot <- bootsample(df_with_mv, 4)
ls_jack <- jacksample(df_with_mv, 4)
```




```{r dict_cat}
dict_name_cat <- dict_onehot(df_with_mv, c(7:8))
```


```{r imputation_boot_FAMD}
# Imputation (for example PCA with 3 dimension)
df_with_mv <- ordinal_encode(df_with_mv, c(7:8))
df_with_mv <- factor_encode(df_with_mv, c(7:8))
ls.imp.pca.onehot <- list()
ls.imp.pca.fact <- list()
idx_col_cat <- c(7:8)
i <- 1
num_col <- ncol(df_with_mv)
n_opt <- 3
for (df in ls_boot) {
  df <- factor_encode(df, idx_col_cat)
  # n_opt = estim_ncpFAMD_mod(df, method.cv="Kfold", verbose=F, maxiter=100)$ncp
  # for imputeFAMD_mod, the categorical data need to be ordinal encoded then factor encoded
  pca <- imputeFAMD_mod(df, ncp = n_opt, maxiter = 30)
  ls.imp.pca.onehot[[i]] <- data.frame(pca$tab.disj)
  ls.imp.pca.fact[[i]] <- pca$completeObs
  i <- i + 1
  break
}
```




```{r}
ls_MSE(X.complete, ls.imp.pca.fact, mask = rs.mix$mar1$R.mask, col_num = c(1:6), resample_method = "bootstrap")
ls_F1(X.complete, ls.imp.pca.fact, mask = rs.mix$mar1$R.mask, col_cat = c(7:8), resample_method = "bootstrap", combine_method = "factor")
ls_F1(X.complete, ls.imp.pca.onehot, mask = rs.mix$mar1$R.mask, col_cat = c(7:20), resample_method = "bootstrap", combine_method = "onehot", dict_cat = dict_name_cat)
head(ls.imp.pca.onehot[[1]])
```


```{r combine_boot_FAMD}
rs0 <- combine_boot(ls.imp.pca.fact, col_con = c(1:5), col_dis = c(6), col_cat = c(7:8), num_row_origin = 10000, method = "factor", dict_cat = dict_name_cat, var_cat = "wilcox_va")
rs <- combine_boot(ls.imp.pca.onehot, col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), num_row_origin = 10000, method = "onehot", dict_cat = dict_name_cat, var_cat = "unalike")
```



```{r imputation_jack_FAMD}
# Imputation for ls_jack (for example PCA with 3 dimension)
ls.imp.pca.onehot <- list()
ls.imp.pca.fact <- list()
i <- 1
num_col <- ncol(df_with_mv)
for (df in ls_jack) {
  df <- factor_encode(df, idx_col_cat)
  # Input: an incomplete dataset
  # Outout: an imputed dataset (onehot or factor for categorical)
  pca <- imputeFAMD_mod(df, ncp = 3, maxiter = 30)
  ls.imp.pca.onehot[[i]] <- data.frame(pca$tab.disj)
  ls.imp.pca.fact[[i]] <- pca$completeObs
  i <- i + 1
}
```

```{r imputation_jack_full_dataframe_FAMD}
df_full <- df_with_mv
df_full <- factor_encode(df_with_mv, idx_col_cat)
pca_full <- imputeFAMD_mod(df_full, ncp = 3, maxiter = 30)
imp.pca_full.onehot <- pca_full$tab.disj
imp.pca_full.fact <- pca_full$completeObs
```


```{r}
ls_MSE(X.complete, ls.imp.pca.fact, mask = rs.mix$mar1$R.mask, col_num = c(1:6), resample_method = "jackknife", df_imp_full = imp.pca_full.fact)
```



```{r combine_jack_FAMD}
rs_jack0 <- combine_jack(ls.imp.pca.onehot, data.frame(imp.pca_full.onehot), col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), method = "onehot", dict_cat = dict_name_cat, var_cat = "unalike")
rs_jack1 <- combine_jack(ls.imp.pca.onehot, data.frame(imp.pca_full.onehot), col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), method = "onehot", dict_cat = dict_name_cat, var_cat = "wilcox_va")
```





### Imputation

```{r}
# list_df.con = list(X.mcar.con,X.mar1.con,X.mar2.con,X.mnar1.con,X.mnar2.con)
# list_df.con_dis = list(X.mcar.con_dis,X.mar1.con_dis,X.mar2.con_dis,X.mnar1.con_dis,X.mnar2.con_dis)
# list_df.mix = list(X.mcar.mix,X.mar1.mix,X.mar2.mix,X.mnar1.mix,X.mnar2.mix)
```

#### (1)EM(error in package mix)
```{r}
list_df.mix[[2]] <- factor_encode(list_df.mix[[2]], idx_col_cat)
df_with_mv <- list_df.mix[[2]]
```

```{r} 
prepare_df_for_em <- function(df, col_cat) {
  allcol <- c(1:length(colnames(df)))
  col_rest <- allcol[!allcol %in% col_cat]
  return(df[, c(col_cat, col_rest)])
}
```


```{r}
library(mix)
data(stlouis)
x <- stlouis
# Perform preliminary manipulations on x. The categorical
# variables need to be coded as consecutive positive integers
# beginning with 1.
s <- prelim.mix(x, 3)
# look at missingness patterns
print(s$r)
# Try EM for general location model without restrictions. This
# algorithm converges after 168 iterations.
thetahat1 <- em.mix(s)
# look at the parameter estimates and loglikelihood
print(getparam.mix(s, thetahat1))
print(getparam.mix(s, thetahat1, corr = T))
print(loglik.mix(s, thetahat1))
# take 100 steps of data augmentation starting from thetahat1
rngseed(1234567)
newtheta <- da.mix(s, thetahat1, steps = 100, showits = T)
# re-run em beginning from newtheta; should converge after 86
# iterations.
thetahat2 <- em.mix(s, newtheta)
# Notice that the loglikelihood at thetahat2 is somewhat different
# than at thetahat1. Examination of thetahat1 and thetahat2 reveals
# that EM has converged to different values. The likelihood is
# multimodal.
print(loglik.mix(s, thetahat2))
print(getparam.mix(s, thetahat2))
# Now try fitting a model with restrictions. We'll first fit the
# "null model" described on p. 130 of Schafer (1991), which
# fits the margins G and D1xD2 in the contingency table, and
# has a full D1*D2 interaction for each continuous variable
# but no effect for G.
margins <- c(1, 0, 2, 3)
intercept <- rep(1, 12)
d1 <- c(-1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1)
d2 <- c(-1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1)
design <- cbind(intercept, d1, d2, d1d2 = d1 * d2)
rm(intercept, d1, d2)
thetahat3 <- ecm.mix(s, margins, design)
print(loglik.mix(s, thetahat3))
# If we play around with starting values, we'll find that the
# likelihood under the "null model" is also multimodal.
# Now let's fit the "alternative model" on p. 131 of Schafer (1991).
margins <- c(1, 2, 0, 2, 3, 0, 1, 3)
glin <- c(-1, 0, 1, -1, 0, 1, -1, 0, 1, -1, 0, 1)
design <- cbind(design, glin)
thetahat4 <- ecm.mix(s, margins, design)
print(loglik.mix(s, thetahat4))
# Now try some imputations. The following commands produce three
# multiple imputations under the alternative model. The imputations
# are proper if we can assume that the data augmentation procedure
# achieves stationarity by 100 steps.
rngseed(454545)
newtheta <- dabipf.mix(s, margins, design, thetahat4, steps = 100, showits = T)
imp1 <- imp.mix(s, newtheta, x)
newtheta <- dabipf.mix(s, margins, design, newtheta, steps = 100, showits = T)
imp2 <- imp.mix(s, newtheta, x)
newtheta <- dabipf.mix(s, margins, design, newtheta, steps = 100, showits = T)
imp3 <- imp.mix(s, newtheta, x)
```


```{r}
# normal assumption in em.mix
# The categorical columns must be ordinal encoded and they must be the first columns of the dataframe
df_for_em <- prepare_df_for_em(df_with_mv, c(7:8))
s <- prelim.mix(df_for_em, 2) # do preliminary manipulations
thetahat <- em.mix(s) # ML estimate for unrestricted model
rngseed(12345) # set random number generator seed
# newtheta <- da.mix(s,thetahat,steps=100) # data augmentation
ximp <- imp.mix(s, thetahat, df_for_em) # impute under newtheta
```


#### (2)EM+Importance resampling
```{r}
# EMB
# multivariate normal assumption
df_comp <- X.complete.ord
df_with_mv <- list_df.mix[[2]]
mask <- rs.mix$mar1$R.mask
col_num <- c(1:6)
col_cat <- c(7:8)
num_imp <- 5 # number of imputations

# imputation
imp_amelia <- amelia(df_with_mv, m = num_imp, p2s = 0, ords = col_cat)
all.matrix <- abind(imp_amelia$imputations, along = 3)
# average the imputation results
result_imp_em_mi <- data.frame(apply(all.matrix, c(1, 2), mean))
# for categorical columns, we round the category to the nearest one (in the situation of ordinal encoding)
result_imp_em_mi[, col_cat] <- round(result_imp_em_mi[, col_cat])
variance_imp_em_mi <- data.frame(apply(all.matrix, c(1, 2), var))

# performance
list_df.mix[[2]]
result_imp_em_mi
variance_imp_em_mi
dens_comp(df_comp[, col_num], result_imp_em_mi[, col_num])
ls_MSE(df_comp, imp_amelia$imputations, mask, col_num)
ls_F1(df_comp, imp_amelia$imputations, mask, col_cat)
```


#### (3)Missforest
```{r}
list_df.mix[[2]] <- factor_encode(list_df.mix[[2]], idx_col_cat)
df_with_mv <- list_df.mix[[2]]
ls_boot <- bootsample(df_with_mv, 4)
ls_jack <- jacksample(df_with_mv, 4)
```

```{r}
# Imputation
ls.imp.forest.boot.onehot <- list()
ls.imp.forest.boot.fact <- list()
idx_col_cat <- c(7:8)
i <- 1
num_col <- ncol(df_with_mv)
for (df in ls_boot) {
  df <- factor_encode(df, idx_col_cat)
  imp.forest <- missForest_mod(xmis = df, maxiter = 3, ntree = 100, col_cat = idx_col_cat)
  ls.imp.forest.boot.onehot[[i]] <- imp.forest$ximp.disj
  ls.imp.forest.boot.fact[[i]] <- imp.forest$ximp
  i <- i + 1
}
```


```{r}
dict_lev <- dict_level(X.complete, c(7:8))
```


```{r}
ls_MSE(X.complete, ls.imp.forest.boot.fact, mask = rs.mix$mar1$R.mask, col_num = c(1:6), resample_method = "bootstrap")
ls_F1(X.complete, ls.imp.forest.boot.fact, mask = rs.mix$mar1$R.mask, dict_lev = dict_lev, col_cat = c(7:8), resample_method = "bootstrap", combine_method = "factor")
ls_F1(X.complete, ls.imp.forest.boot.onehot, mask = rs.mix$mar1$R.mask, dict_lev = dict_lev, col_cat = c(7:20), resample_method = "bootstrap", combine_method = "onehot", dict_cat = dict_name_cat)
```




```{r}
rs0 <- combine_boot(ls.imp.forest.boot.fact, col_con = c(1:5), col_dis = c(6), col_cat = c(7:8), num_row_origin = 10000, method = "factor", dict_cat = dict_name_cat, var_cat = "wilcox_va")
rs <- combine_boot(ls.imp.forest.boot.onehot, col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), num_row_origin = 10000, method = "onehot", dict_cat = dict_name_cat, var_cat = "unalike")
```

```{r}
# Imputation for ls_jack (for example PCA with 3 dimension)
ls.imp.forest.jack.onehot <- list()
ls.imp.forest.jack.fact <- list()
i <- 1
num_col <- ncol(df_with_mv)
for (df in ls_jack) {
  df <- factor_encode(df, idx_col_cat)
  # Input: an incomplete dataset
  # Outout: an imputed dataset (onehot or factor for categorical)
  imp.forest <- missForest_mod(xmis = df, maxiter = 5, ntree = 100, col_cat = idx_col_cat)
  ls.imp.forest.jack.onehot[[i]] <- imp.forest$ximp.disj
  ls.imp.forest.jack.fact[[i]] <- imp.forest$ximp
  i <- i + 1
}
```

```{r}
df_full <- df_with_mv
df_full <- factor_encode(df_with_mv, idx_col_cat)
imp.forest_full <- missForest_mod(xmis = df_full, maxiter = 5, ntree = 100, col_cat = idx_col_cat)
ls.imp.forest_full.jack.onehot <- imp.forest_full$ximp.disj
ls.imp.forest_full.jack.fact <- imp.forest_full$ximp
```


```{r}
ls_MSE(X.complete, ls.imp.forest.jack.fact, mask = rs.mix$mar1$R.mask, col_num = c(1:6), resample_method = "jackknife", df_imp_full = ls.imp.forest_full.jack.fact)
```


```{r}
rs_jack0 <- combine_jack(ls.imp.forest.jack.onehot, data.frame(ls.imp.forest_full.jack.onehot), col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), method = "onehot", dict_cat = dict_name_cat, var_cat = "unalike")
rs_jack1 <- combine_jack(ls.imp.forest.jack.onehot, data.frame(ls.imp.forest_full.jack.onehot), col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), method = "onehot", dict_cat = dict_name_cat, var_cat = "wilcox_va")
rs_jack0
rs_jack1
```




















```{r}

imp.forest <- missForest(xmis = list_df.mix[[2]], maxiter = 5, ntree = 100, dict_cat = dict_name_cat)
imp.forest$ximp
imp.forest$ximp.disj
```

```{r}
ls_MSE(df_comp, list(imp.forest$ximp), mask, col_num)
ls_F1(df_comp, list(imp.forest$ximp), mask, col_cat)
```


#### (4)MissRanger
```{r}
list_df.mix[[2]] <- factor_encode(list_df.mix[[2]], idx_col_cat)
df_with_mv <- list_df.mix[[2]]
```




```{r}

res <- missRanger_mod(df_with_mv, col_cat = c(7:8))
```


```{r}
# head(res$ximp,10)
# head(res$ximp.disj,10)
```





#### (5)MissMDA

```{r}
list_df.mix[[2]] <- factor_encode(list_df.mix[[2]], idx_col_cat)
ncp.pca <- estim_ncpFAMD(list_df.mix[[2]], method.cv = "Kfold", verbose = T, maxiter = 50)$ncp
pca <- imputeFAMD(list_df.mix[[2]], ncp = 5)
imp.pca <- pca$comp
```

```{r}
imp.pca <- ordinal_encode(imp.pca, idx_col_cat)
imp.pca
ls_MSE(df_comp, list(imp.pca), mask, col_num)
ls_F1(df_comp, list(imp.pca), mask, col_cat)
```





#### (6)KNN
```{r}
list_df.mix[[2]] <- factor_encode(list_df.mix[[2]], c(7:8))
df_with_mv <- list_df.mix[[2]]
```




```{r}
for (df in ls_boot) {
  res <- kNN_mod(df, col_cat = c(7:8))
}
```


```{r}
# head(res$ximp)
# head(res$ximp.disj)
# head(res$R.mask)
```





















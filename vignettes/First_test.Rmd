---
title: "First Test"
output:
  rmarkdown::html_vignette
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
---

```{r dependencies, message=FALSE, warning=FALSE}
# library(MASS)
# library(norm)
# library(VIM)
# # library(ggplot2)
# # library(ggpubr)
# library(naniar)
# library(usethis)
# # library(MissMech) # removed from the CRAN repository.
# # library(BaylorEdPsych) # removed from the CRAN repository.
# # library(mvnmle) # cannot find
# library(rlist)
# library(boot)
# library(devtools)
# library(Amelia)
# library(abind)
# library(missMethods)
# library(gbutils)
# library(pracma)
# library(gdata)
# library(caret)
# library(stats)
# library(missMDA)
# library(uwo4419)
# library(qualvar)
# library(dplyr)
# library(data.table)
# library(mix)
# library(ranger)
# library(FactoMineR)
# # source_url('https://raw.githubusercontent.com/R-miss-tastic/website/master/static/how-to/generate/amputation.R')
# # source_url('https://raw.githubusercontent.com/njtierney/naniar/master/R/mcar-test.R')
# # source_url('https://raw.githubusercontent.com/njtierney/naniar/master/R/utils.R')
# # source_url('https://raw.githubusercontent.com/LqNoob/Machine-Learning-Evaluation-Metrics/master/R/Classification.R')
# 
# 
# 
# library(MissImp)
# # source("../R/utils.R")
# # source("../R/generate_missingness.R")
# # source("../R/produce_NA.R")
# # source("../R/dummy_test_MCAR.R")
# # source("../R/MissMech_TestNormality.R")
# # source("../R/miss_ranger_function.R")
# # source("../R/bootstrap.R")
# # source("../R/jackknife.R")
# # source("../R/missMDA_function.R")
# # source("../R/mcar_test_combined.R")
# # source("../R/VIM_kNN.R")
# 
# # set.seed(43)
```


### Generate data
#### Complete data
We generate a complete data set (Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8) with (Y1, Y2, Y3) ~ N(u1, S1), (Y4, Y5)~N(u2,S2), Y6~P(\lambda) and Y7,Y8~Binomial(prob). 


```{r Generate_complete_dataframe}
n <- 10000
complete_df_generator <- function(n){
  mu.X <- c(1, 2, 3)
  Sigma.X <- matrix(c(
  9, 3, 2,
  3, 4, 0,
  2, 0, 1
  ), nrow = 3)
  X.complete.cont <- MASS::mvrnorm(n, mu.X, Sigma.X) # multivariate normal distribution

  mu1.X <- c(9, 8)
  Sigma1.X <- matrix(c(
  16, 14,
  14, 25
  ), nrow = 2)

  X.complete.cont1 <- MASS::mvrnorm(n, mu1.X, Sigma1.X) # multivariate normal distribution

  lambda <- 4.3
  X.complete.discr <- stats::rpois(n, lambda) # poisson distribution

  X.complete.cat <- stats::rbinom(n, size = 5, prob = 0.4) # binomial

  X.complete.cat2 <- stats::rbinom(n, size = 7, prob = 0.6) # binomial

  X.complete <- data.frame(cbind(X.complete.cont, X.complete.cont1,
                                 X.complete.discr, ... = X.complete.cat, X.complete.cat2))
  X.complete[, 7] <- as.factor(X.complete[, 7])
  levels(X.complete[, 7]) <- c("F", "E", "D", "C", "B", "A")
  X.complete[, 8] <- as.factor(X.complete[, 8])
  colnames(X.complete) <- c("Y1", "Y2", "Y3", "Y4", "Y5", "Y6", "Y7", "Y8")
  return(X.complete)
}
X.complete <- complete_df_generator(n)
```



#### Add missingness

```{r add_miss_one_mechanism}
rs = generate_miss(X.complete, 0.4, mechanism = "MNAR1")
```

We generate missing values first on the continuous variables (V1,...,V5), with the result in rs.con; 
on the continous + discrete varaibles (V1,...,V6), with the result in rs.con_dis; and on continous + discrete + categorical variables, with the result in rs.mix.

```{r add_miss_all_mechanism}
rs.con <- generate_miss_ls(X.complete[, 1:5], 0.6)
rs.con_dis <- generate_miss_ls(X.complete[, 1:6], 0.6)
rs.mix <- generate_miss_ls(X.complete, 0.6)
```


```{r list_of_incomplete_dataframes}
list_df.con <- list(rs.con$mcar$X.incomp, rs.con$mar1$X.incomp, rs.con$mar2$X.incomp, rs.con$mar3$X.incomp, rs.con$mnar1$X.incomp, rs.con$mnar2$X.incomp)

list_df.con_dis <- list(rs.con_dis$mcar$X.incomp, rs.con_dis$mar1$X.incomp, rs.con_dis$mar2$X.incomp, rs.con_dis$mar3$X.incomp, rs.con_dis$mnar1$X.incomp, rs.con_dis$mnar2$X.incomp)

list_df.mix <- list(rs.mix$mcar$X.incomp, rs.mix$mar1$X.incomp, rs.mix$mar2$X.incomp, rs.mix$mar3$X.incomp, rs.mix$mnar1$X.incomp, rs.mix$mnar2$X.incomp)
```



### Statistical test for MCAR 
```{r p_val}
p_val <- 0.05
```

```{r p_val_one_df}
# t <- mcar_test_combined(list_df.mix[[5]], col_cat = c(7,8), p_val = p_val)
```

```{r p_val_for_each_incomplete_data_with_each_test}
# A dataframe that record th p-values for each test
test_p_vals <- data.frame(matrix(rep(0,4),nrow=1)) 
colnames(test_p_vals) <- c("Dummy.variable.test","Little.s.MCAR.test","Hawkin.s.test","Non.parametric.test")

ls_test <- list("MCAR", "MAR1", "MAR2", "MAR3", "MNAR1", "MNAR2")
i <- 1
t <- 1
col_cat <- c(7:8)
for (ls in list(list_df.con, list_df.con_dis, list_df.mix)) {
  for (df in ls) {
    if (i == 3) {
      out <- mcar_test_combined(df, col_cat, p_val)
    }
    else {
      out <- mcar_test_combined(df, c(), p_val)
    }
    test_p_vals[t, ] <- out$p_values[1,]
    t <- t + 1
  }
  i <- i + 1
}
test_p_vals[["type"]] <- c(rep("continuous",6), rep("continuous+discrete",6),rep("mix",6))
test_p_vals[["mechanism"]] <- rep(c("MCAR", "MAR1", "MAR2", "MAR3", "MNAR1", "MNAR2"),3)
test_results <- data.frame(test_p_vals[,c(1,2)]>p_val)
test_results[["Miss_Mech_test"]] = c((test_p_vals[["Hawkin.s.test"]] < p_val) * (test_p_vals[["Non.parametric.test"]] < p_val) == 0)

test_p_vals
test_results
# The result of MissMech test is not very accurate
```

#### Test with dummy variables
```{r dummy_test}
# # dummy_test(rs.con$X.mcar,c())
# test_result_dum <- data.frame() # A dataframe that record th p-values for each test
# ls_test <- list("MCAR", "MAR1", "MAR2", "MAR3", "MNAR1", "MNAR2")
# i <- 1
# col_cat <- c(7:8)
# for (ls in list(list_df.con, list_df.con_dis, list_df.mix)) {
#   j <- 1
#   for (df in ls) {
#     if (i == 3) {
#       out <- dummy_test(df, col_cat)
#     }
#     else {
#       out <- dummy_test(df, c())
#     }
#     test_result_dum[i, ls_test[[j]]] <- out$p.value
#     j <- j + 1
#   }
#   i <- i + 1
# }
# row.names(test_result_dum) <- c("Continuous", "Continuous+Discret", "Continous+Discret+Categorical")
# print(test_result_dum)
# 
# final_result_mcar <- data.frame(test_result_dum > p_val)
# final_result_mcar
```




#### Test with Little's MCAR test

```{r little's_MCAR_test}
# test_result_little <- data.frame() # A dataframe that record th p-values for each test
# ls_test <- list("MCAR", "MAR1", "MAR2", "MAR3", "MNAR1", "MNAR2")
# i <- 1
# for (ls in list(list_df.con, list_df.con_dis, list_df.mix)) {
#   j <- 1
#   for (df in ls) {
#     out <- mcar_test(df)
#     test_result_little[i, ls_test[[j]]] <- out$p.value
#     j <- j + 1
#   }
#   i <- i + 1
# }
# row.names(test_result_little) <- c("Continuous", "Continuous+Discret", "Continous+Discret+Categorical")
# print(test_result_little)
# 
# final_result_mcar <- data.frame(test_result_little > p_val)
# final_result_mcar
```

#### Test with Hawkin's test and nonparametric test
It is slower than mcar_test.
I find the result is less reliable than the mcar_test, and it couldn't deal with large number of data

```{r missMech_test}
# test_result_hawkins <- data.frame() # A dataframe that record th p-values for each Hawkin's test
# test_result_nonparam <- data.frame() # A dataframe that record th p-values for each nonparametric test
# ls_test <- list("MCAR", "MAR1", "MAR2", "MAR3", "MNAR1", "MNAR2")
# i <- 1
# for (ls in list(list_df.con, list_df.con_dis, list_df.mix)) {
#   j <- 1
#   for (df in ls) {
#     out_missmech <- TestMCARNormality(df)
#     test_result_hawkins[i, ls_test[[j]]] <- out_missmech$pvalcomb
#     test_result_nonparam[i, ls_test[[j]]] <- out_missmech$pnormality
#     j <- j + 1
#   }
#   i <- i + 1
# }
# 
# row.names(test_result_hawkins) <- c("Continuous", "Continuous+Discret", "Continous+Discret+Categorical")
# row.names(test_result_nonparam) <- c("Continuous", "Continuous+Discret", "Continous+Discret+Categorical")
# 
# test_result_hawkins
# test_result_nonparam
# 
# final_result_mcar <- data.frame(test_result_hawkins < p_val) * data.frame(test_result_nonparam < p_val)
# final_result_mcar <- data.frame(final_result_mcar == 0)
# final_result_mcar
```






### Summary of imputation

```{r imputation}
#test numeric database with every method
# imp_method <- match.arg(imp_method, c("missRanger", "kNN", "missForest", "PCA", "EM", "MI_EM", "MI_PCA", "MICE", "MI_Ranger","MI_Ranger_bis"))

n <- 10000
mech <- 'MAR2'
miss_prop <- 0.4
X.complete <- complete_df_generator(n)[,c(1:6)]
rs <- generate_miss(X.complete, miss_prop, mechanism = mech)
df <- rs$X.incomp
col_cat <- c()
col_dis <- c(6)
n_resample <- 3#2*round(log(nrow(df)))
maxiter_tree <- 3
maxiter_pca <- 3
maxiter_mice <- 10
ncp_pca <- 3
learn_ncp <- FALSE
df_complete <- X.complete
num_mi <- 2

imp_method <- "MI_PCA"
resample_method <- "jackknife"
cat_combine_by <- "onehot"
var_cat <- "unalike"
```




```{r}
res <- single_imp(
  df = df, imp_method = imp_method, resample_method = resample_method,
  n_resample = n_resample, col_cat = col_cat, col_dis = col_dis,
  maxiter_tree = maxiter_tree, maxiter_pca = maxiter_pca, ncp_pca = ncp_pca,
  learn_ncp = learn_ncp, cat_combine_by = cat_combine_by, var_cat = var_cat,
  df_complete = df_complete, num_mi=num_mi, maxiter_mice = maxiter_mice
)
```



```{r}
head(df)
head(res$imp)
head(res$imp.disj)
head(res$uncertainty)
head(res$uncertainty.disj)
res$MSE
res$F1
# dens_comp(res$imp,X.complete)
```

```{r}
sum(res$uncertainty, na.rm = TRUE)/(rs$real_miss_perc*10000*8)
#0.3: 2.15438
#0.5: 1.654947
#0.7: 1.210844
```

```{r}
# res_mean_imp <- df
# col_num <- c(1:6)
# col_num_name <-  colnames(df)[col_num]
# for(y in col_num_name){
#   res_mean_imp[[y]][is.na(res_mean_imp[[y]])] <- mean(res_mean_imp[[y]],na.rm=TRUE)
# }
# 
# 
# ycol <- "Y3"
# X.miss <- X.complete
# X.miss[!is.na(df)] <- NA
# dat <- data.frame(
#       y = c(X.complete[[ycol]], res$imp[[ycol]],res_mean_imp[[ycol]]),
#       lines = rep(c("complete", "imputed", "mean_imputed"), each = 10000)
#     )
# dat <- data.frame(
#       y = c(X.miss[[ycol]]-res$imp[[ycol]], X.miss[[ycol]]-res_mean_imp[[ycol]]),
#       lines = rep(c("imputed_error", "mean_imputed_error"), each = 10000)
#     )
# dat <- data.frame(
#       y = c(X.miss[[ycol]], df[[ycol]]),
#       lines = rep(c("missing", "observed"), each = 10000)
#     )
# ggplot2::ggplot(dat, ggplot2::aes(x = y, fill = lines)) +
#       ggplot2::geom_density(alpha = 0.3) +
#       ggplot2::labs(x = ycol)
# show(p)
```










### Single Imputation
#### (1)EM
```{r}
df_with_mv <- list_df.mix[[2]]
df_with_mv <- ordinal_encode(df_with_mv, c(7,8))
df_with_mv <- factor_encode(df_with_mv, c(7,8))
ls_boot <- bootsample(df_with_mv, 4)
ls_jack <- jacksample(df_with_mv, 4)
dict_lev <- dict_level(df_with_mv, c(7:8))
```



```{r}
# Imputation for ls_jack (for example PCA with 3 dimension)
ls.imp.em.jack.onehot <- list()
ls.imp.em.jack.fact <- list()
i <- 1
num_col <- ncol(df_with_mv)
for (df in ls_jack) {
  #df <- factor_encode(df, idx_col_cat)
  # Input: an incomplete dataset
  # Outout: an imputed dataset (onehot or factor for categorical)
  imp.em <- em_mod(df, col_cat = col_cat)
  
  ls.imp.em.jack.onehot[[i]] <- imp.em$ximp.disj
  ls.imp.em.jack.fact[[i]] <- imp.em$ximp
  i <- i + 1
}
```
```{r}
df = ls_jack[[2]]
imp.em <- em_mod(df, col_cat = idx_col_cat)
```





#### (2)Missforest
```{r}
list_df.mix[[2]] <- factor_encode(list_df.mix[[2]], idx_col_cat)
df_with_mv <- list_df.mix[[2]]
ls_boot <- bootsample(df_with_mv, 4)
ls_jack <- jacksample(df_with_mv, 4)
```

```{r}
# Imputation
ls.imp.forest.boot.onehot <- list()
ls.imp.forest.boot.fact <- list()
idx_col_cat <- c(7:8)
i <- 1
num_col <- ncol(df_with_mv)
for (df in ls_boot) {
  df <- factor_encode(df, idx_col_cat)
  imp.forest <- missForest_mod(xmis = df, maxiter = 3, ntree = 100, col_cat = idx_col_cat)
  ls.imp.forest.boot.onehot[[i]] <- imp.forest$ximp.disj
  ls.imp.forest.boot.fact[[i]] <- imp.forest$ximp
  i <- i + 1
}
```


```{r}
dict_lev <- dict_level(df_with_mv, c(7:8))
```


```{r}
ls_MSE(X.complete, ls.imp.forest.boot.fact, mask = rs.mix$mar1$R.mask, col_num = c(1:6), resample_method = "bootstrap")
ls_F1(X.complete, ls.imp.forest.boot.fact, mask = rs.mix$mar1$R.mask, dict_lev = dict_lev, col_cat = c(7:8), resample_method = "bootstrap", combine_method = "factor")
ls_F1(X.complete, ls.imp.forest.boot.onehot, mask = rs.mix$mar1$R.mask, dict_lev = dict_lev, col_cat = c(7:20), resample_method = "bootstrap", combine_method = "onehot", dict_cat = dict_name_cat)
```




```{r}
rs0 <- combine_boot(ls.imp.forest.boot.fact, col_con = c(1:5), col_dis = c(6), col_cat = c(7:8), num_row_origin = 10000, method = "factor", dict_cat = dict_name_cat, var_cat = "wilcox_va")
rs <- combine_boot(ls.imp.forest.boot.onehot, col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), num_row_origin = 10000, method = "onehot", dict_cat = dict_name_cat, var_cat = "unalike")
```

```{r}
# Imputation for ls_jack (for example PCA with 3 dimension)
ls.imp.forest.jack.onehot <- list()
ls.imp.forest.jack.fact <- list()
i <- 1
num_col <- ncol(df_with_mv)
for (df in ls_jack) {
  df <- factor_encode(df, idx_col_cat)
  # Input: an incomplete dataset
  # Outout: an imputed dataset (onehot or factor for categorical)
  imp.forest <- missForest_mod(xmis = df, maxiter = 5, ntree = 100, col_cat = idx_col_cat)
  ls.imp.forest.jack.onehot[[i]] <- imp.forest$ximp.disj
  ls.imp.forest.jack.fact[[i]] <- imp.forest$ximp
  i <- i + 1
}
```

```{r}
df_full <- df_with_mv
df_full <- factor_encode(df_with_mv, idx_col_cat)
imp.forest_full <- missForest_mod(xmis = df_full, maxiter = 5, ntree = 100, col_cat = idx_col_cat)
ls.imp.forest_full.jack.onehot <- imp.forest_full$ximp.disj
ls.imp.forest_full.jack.fact <- imp.forest_full$ximp
```


```{r}
ls_MSE(X.complete, ls.imp.forest.jack.fact, mask = rs.mix$mar1$R.mask, col_num = c(1:6), resample_method = "jackknife", df_imp_full = ls.imp.forest_full.jack.fact)
```


```{r}
rs_jack0 <- combine_jack(ls.imp.forest.jack.onehot, data.frame(ls.imp.forest_full.jack.onehot), col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), method = "onehot", dict_cat = dict_name_cat, var_cat = "unalike")
rs_jack1 <- combine_jack(ls.imp.forest.jack.onehot, data.frame(ls.imp.forest_full.jack.onehot), col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), method = "onehot", dict_cat = dict_name_cat, var_cat = "wilcox_va")
rs_jack0
rs_jack1
```




















```{r}

imp.forest <- missForest(xmis = list_df.mix[[2]], maxiter = 5, ntree = 100, dict_cat = dict_name_cat)
imp.forest$ximp
imp.forest$ximp.disj
```

```{r}
ls_MSE(df_comp, list(imp.forest$ximp), mask, col_num)
ls_F1(df_comp, list(imp.forest$ximp), mask, col_cat)
```


#### (3)MissRanger
```{r}
list_df.mix[[2]] <- factor_encode(list_df.mix[[2]], idx_col_cat)
df_with_mv <- list_df.mix[[2]]
```




```{r}

res <- missRanger_mod(df_with_mv, col_cat = c(7:8))
```


```{r}
# head(res$ximp,10)
# head(res$ximp.disj,10)
```





#### (4)MissMDA


```{r bootstrap_and_jackknife}
# Create bootstrap and jackknife datasets
# Bootstrap needs big number of samples
# Jackknife has a problem with categorical variables
df_with_mv <- list_df.mix[[2]]
df_with_mv = ordinal_encode(df_with_mv,c(7:8))
df_with_mv <- factor_encode(df_with_mv, c(7:8))
ls_boot <- bootsample(df_with_mv, 7)
ls_jack <- jacksample(df_with_mv, 7)
```



```{r dict_cat}
dict_name_cat <- dict_onehot(df_with_mv, c(7:8))
```


```{r imputation_boot_FAMD}

#Imputation (for example PCA with 3 dimension)
df_with_mv = ordinal_encode(df_with_mv,c(7:8))
df_with_mv = factor_encode(df_with_mv,c(7:8))
ls.imp.pca.onehot = list()
ls.imp.pca.fact = list()
idx_col_cat = c(7:8)
i = 1
num_col = ncol(df_with_mv)
n_opt =3
for(df in ls_boot){
  df = factor_encode(df, idx_col_cat)
  #n_opt = estim_ncpFAMD_mod(df, method.cv="Kfold", verbose=F, maxiter=100)$ncp
  #for imputeFAMD_mod, the categorical data need to be ordinal encoded then factor encoded
  pca = imputeFAMD_mod(df, ncp = n_opt, maxiter = 30)
  ls.imp.pca.onehot[[i]] = data.frame(pca$tab.disj)
  ls.imp.pca.fact[[i]] = pca$completeObs
  i = i + 1
}
```



```{r combine_boot_FAMD}
rs0 <- combine_boot(ls.imp.pca.fact, col_con = c(1:5), col_dis = c(6), col_cat = c(7:8), num_row_origin = 10000, method = "factor", dict_cat = dict_name_cat, var_cat = "wilcox_va")
rs <- combine_boot(ls.imp.pca.onehot, col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), num_row_origin = 10000, method = "onehot", dict_cat = dict_name_cat, var_cat = "unalike")
```



```{r imputation_jack_FAMD}
# Imputation for ls_jack (for example PCA with 3 dimension)
ls.imp.pca.onehot <- list()
ls.imp.pca.fact <- list()
i <- 1
num_col <- ncol(df_with_mv)
for (df in ls_jack) {
  df <- factor_encode(df, c(7,8))
  # Input: an incomplete dataset
  # Outout: an imputed dataset (onehot or factor for categorical)
  pca <- imputeFAMD_mod(df, ncp = 3, maxiter = 30)
  ls.imp.pca.onehot[[i]] <- data.frame(pca$tab.disj)
  ls.imp.pca.fact[[i]] <- pca$completeObs
  i <- i + 1
}
```

```{r imputation_jack_full_dataframe_FAMD}
df_full <- df_with_mv
df_full <- factor_encode(df_with_mv, c(7,8))
pca_full <- imputeFAMD_mod(df_full, ncp = 3, maxiter = 30)
imp.pca_full.onehot <- pca_full$tab.disj
imp.pca_full.fact <- pca_full$completeObs
```




```{r combine_jack_FAMD}
rs_jack0 <- combine_jack(ls.imp.pca.onehot, data.frame(imp.pca_full.onehot), col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), method = "onehot", dict_cat = dict_name_cat, var_cat = "unalike")
rs_jack1 <- combine_jack(ls.imp.pca.onehot, data.frame(imp.pca_full.onehot), col_con = c(1:5), col_dis = c(6), col_cat = c(7:20), method = "onehot", dict_cat = dict_name_cat, var_cat = "wilcox_va")
```







#### (5)KNN
```{r}
list_df.mix[[2]] <- factor_encode(list_df.mix[[2]], c(7:8))
df_with_mv <- list_df.mix[[2]]
```




```{r}
for (df in ls_boot) {
  res <- kNN_mod(df, col_cat = c(7:8))
}
```


```{r}
# head(res$ximp)
# head(res$ximp.disj)
# head(res$R.mask)
```





















### Multiple Imputation
#### (1)MI EM(need change)
```{r}
# df_with_mv <- list_df.mix[[2]]
df_with_mv <- rs$X.incomp
df_with_mv <- factor_ordinal_encode(df_with_mv, c(7,8))
dict_lev <- dict_level(df_with_mv, c(7:8))
```



```{r}
em_mi_mod <- function(df, col_cat){
  exist_cat <- !all(c(0, col_cat) == c(0))
  if(!exist_cat){# if there are only numerical columns
    s <- mix::prelim.mix(df, 0) # do preliminary manipulations
    thetahat <- mix::em.mix(s) # ML estimate for unrestricted model
    mix::rngseed(43) # set random number generator seed
    newtheta <- mix::da.mix(s,thetahat,steps=100)
    ximp <- mix::imp.mix(s, thetahat, df) # impute under newtheta (one draw)
    return(list(ximp=ximp, ximp.disj=ximp))
  }
  
  dict_name_cat <- dict_onehot(df, col_cat)
  df.cat <- df[,col_cat]
  #prepare for em and imputation
  # The categorical columns must be ordinal encoded and they must be the first columns of the dataframe
  df_for_em <- prepare_df_for_em(df, col_cat)
  s <- mix::prelim.mix(df_for_em, length(col_cat)) # do preliminary manipulations
  thetahat <- mix::em.mix(s) # ML estimate for unrestricted model
  mix::rngseed(43) # set random number generator seed
  newtheta <- mix::da.mix(s, thetahat, steps=100)
  ximp <- mix::imp.mix(s, thetahat, df_for_em) # impute under newtheta (one draw)
  ximp <- factor_encode(data.frame(ximp), c(1:length(col_cat)))
  #rearrange columns
  cols <- colnames(df)
  ximp <- ximp[cols]
  
  #create disjonctive table
  dims <- c()
  dimnames <- list()
  dummy <- dummyVars(" ~ .", data = ximp, sep = "_")
  ximp.disj <- data.frame(predict(dummy, newdata = ximp))
  
  #construction of tensor pi
  i <- 1
  for(name in names(dict_name_cat)){
    #ximp.disj[[name]] <- df_with_mv[[name]]
    lev <- dict_name_cat[[name]]
    dims <- c(dims,length(lev))
    dimnames[[i]] <- lev
    i <- i + 1
  }
  pi <- thetahat$pi
  tensor_pi <- array(pi, dims, dimnames)
  
  #ximp.disj onehot probability
  cat_name <- unlist(dict_name_cat)
  ximp.disj[cat_name] <- t(apply(as.matrix(df.cat),1,
                                 FUN=function(x) prob_vector_cat(x, tensor_pi,dict_name_cat)))
  return(list(ximp=data.frame(ximp), ximp.disj=data.frame(ximp.disj)))
}
```




#### (2)MI EM Amelia
```{r}
# EMB (with bootstrap option, but here we've turned it off)
# multivariate normal assumption
df_comp <- X.complete
# df_with_mv <- list_df.mix[[2]]
mask <- data.frame(is.na(df_with_mv))

res<- MI_em_amelia(df_with_mv, col_num=c(1:6), col_cat=c(7:8), num_imp=10)


# performance
# list_df.mix[[2]]
# result_imp_em_mi
# variance_imp_em_mi
# dens_comp(df_comp[, col_num], result_imp_em_mi[, col_num])
ls_MSE(df_comp, res$ls_ximp, mask, c(1:6), resample_method = "mi")
#ls_F1(df_comp, imp_amelia$imputations, mask, col_cat)
```


```{r}
head(res$ximp)
head(res$ximp.disj)


```



#### (3) MI mice
```{r}
df_comp <- X.complete
# df_with_mv <- list_df.mix[[2]]
imp_num=10
df_with_mv <- generate_miss(df_comp, 0.4, mechanism = "MAR1")$X.incomp
df_with_mv <- factor_ordinal_encode(df_with_mv,c(7,8))
res<-mice(df_with_mv, m=imp_num)
result_mice(res, impnum, col_cat=c(7,8))


    
```


```{r}
complete(res)
```




#### (4) MI PCA
```{r}
df_comp <- X.complete
# df_with_mv <- list_df.mix[[2]]
df_with_mv <- generate_miss(df_comp, 0.4, mechanism = "MAR1")$X.incomp
mask <- data.frame(is.na(df_with_mv))
df_with_mv <- factor_ordinal_encode(df_with_mv,c(7,8))
dict_cat <- dict_onehot(df_with_mv, c(7,8))
res<- MIFAMD_mod(df_with_mv, ncp=3, maxiter = 50, dict_cat=dict_cat)
```


```{r}
res$ximp
res$ximp.disj


```



#### (5) MI MissRanger
```{r}
df_comp <- X.complete
df_with_mv <- generate_miss(df_comp, 0.4, mechanism = "MAR1")$X.incomp
df_with_mv <- factor_ordinal_encode(df_with_mv,c(7,8))
res<- MI_missRanger(df_with_mv, col_cat = c(7,8), num_mi = 5)
```



### Test with different missing proportion

```{r different_miss_prop}
mech <- 'MAR1'
# df_complete <- X.complete
col_cat <- c(7,8)
col_dis <- c(6)
maxiter_tree <- 10
maxiter_pca <- 50
maxiter_mice <- 5
ncp_pca <- 3#round(ncol(df_complete)/2)
learn_ncp <- FALSE

num_mi <- 4
n_df <- 4
n_resample <- 3#2*round(log(nrow(df_complete)))


imp_method <- "PCA"
resample_method <- "bootstrap"
cat_combine_by <- "onehot"
var_cat <- "wilcox_va"

```


```{r different_miss_prop}
i <- 1
result <- list()
for(miss_prop in c(0.01, 0.1, 0.3)){
  print(miss_prop)
  rs <- generate_miss(df_complete, miss_prop, mechanism = mech)
  miss_prop_real <- rs$real_miss_perc
  df_incomp <- rs$X.incomp
  result[["mechanisme"]][i] <- mech
  result[["miss_perc"]][i] <- miss_prop_real
  result[["method"]][i] <- imp_method
  result[["resample"]][i] <- resample_method
  res_imp <- single_imp(df = df_incomp, imp_method = imp_method,
                        resample_method = resample_method, n_resample = n_resample,
                        col_cat = col_cat, col_dis = col_dis,
                        maxiter_tree = maxiter_tree, maxiter_pca = maxiter_pca,
                        ncp_pca = ncp_pca, learn_ncp = learn_ncp,
                        cat_combine_by = cat_combine_by, var_cat = var_cat,
                        df_complete = df_complete, num_mi=num_mi,
                        maxiter_mice = maxiter_mice)
  result[["MSE"]][i] <- res_imp$MSE$Mean_MSE
  result[["Var_MSE"]][i] <- res_imp$MSE$Variance_MSE
  result[["F1"]][i] <- res_imp$F1$Mean_F1
  result[["Var_F1"]][i] <- res_imp$F1$Variance_F1
  i <- i + 1
}

data.frame(result)
```

```{r write_csv}
write.csv(result,"test_MAR1_MIRanger_boot_result.csv")
# read.csv("test_MAR1_MIPCA_boot_result.csv")
```


```{r different_df}
i <- 1
n <- 10000
result <- list()
for(miss_prop in c(0.01, 0.03, 0.07, 0.1, 0.2, 0.3, 0.5, 0.7)){
  print(miss_prop)
  miss_prop_real <- c()
  ls_mse <- c()
  ls_f1 <- c()
  ls_seed <- sample(1:100, n_df, replace=TRUE)
  for(j in seq(n_df)){
    set.seed(ls_seed[j])
    df_complete <- complete_df_generator(n)
    rs <- generate_miss(df_complete, miss_prop, mechanism = mech)
    miss_prop_real <- c(miss_prop_real,rs$real_miss_perc)
    df_incomp <- rs$X.incomp
    res_imp <- single_imp(df = df_incomp, imp_method = imp_method, 
                        resample_method = resample_method, n_resample = n_resample, 
                        col_cat = col_cat, col_dis = col_dis,
                        maxiter_tree = maxiter_tree, maxiter_pca = maxiter_pca, 
                        ncp_pca = ncp_pca, learn_ncp = learn_ncp, 
                        cat_combine_by = cat_combine_by, var_cat = var_cat,
                        df_complete = df_complete, num_mi=num_mi, 
                        maxiter_mice = maxiter_mice)
    ls_mse <- c(ls_mse,res_imp$MSE$Mean_MSE)
    ls_f1 <- c(ls_f1,res_imp$F1$Mean_F1)
  }
  result[["mechanisme"]][i] <- mech
  result[["miss_perc"]][i] <- mean(miss_prop_real)
  result[["method"]][i] <- imp_method
  #result[["resample"]][i] <- resample_method
  result[["MSE"]][i] <- mean(ls_mse)
  result[["Var_MSE"]][i] <- var(ls_mse)
  result[["F1"]][i] <- mean(ls_f1)
  result[["Var_F1"]][i] <- var(ls_f1)
  i <- i + 1
}

data.frame(result)
```



```{r write_csv}
write.csv(result,"test_MAR1_PCA_boot_diff_dfcomp_result.csv")
read.csv("test_MAR1_MIRanger_bis_boot_diff_dfcomp_result.csv")
```


```{r write_csv}
res <- read.csv("test_MAR1_MissRanger_boot_diff_dfcomp_result.csv")
x <- res$miss_perc
y1 <- res$MSE
y2 <- res$MSE + 1.96*res$Var_MSE
y3 <- res$MSE - 1.96*res$Var_MSE
# 
# plot(x, y1, type = "n", ylim = range(c(y1, y2)), xlab = "", ylab = "")
# lines(x, y1, col = "blue")
# lines(x, y2, col = "red")
# lines(x, y3, col = "red") 
predframe <- with(res,data.frame(miss_perc,
                                MSE=MSE,lwr=MSE-1.96*Var_MSE,upr=MSE+1.96*Var_MSE))
p1 <- ggplot(predframe, aes(miss_perc, MSE))+
    geom_point()+
    geom_line(data=predframe)+
    geom_ribbon(data=predframe,aes(ymin=lwr,ymax=upr),alpha=0.3, fill='red')+
    xlab('Percentage of missing data')
show(p1)
```

`



---
title: "Estimating parameters with missing values"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
---

## 1.Preparation
```{r}
suppressPackageStartupMessages(require(MASS))
suppressPackageStartupMessages(require(norm))
suppressPackageStartupMessages(require(mice))
set.seed(1)
```

## 2. Linear regression
### 2.1 Generate data with missing values MCAR
```{r}
# Sample data generation -------------------------------------------------------
mu.X <- c(1, 1)
Sigma.X <- matrix(c(1, 1, 1, 4), nrow = 2)
n <- 1000
X.complete <- mvrnorm(n, mu.X, Sigma.X)
b <- c(2, 3, -1)
sigma.eps <- 0.25
y.complete <- cbind(rep(1, n), X.complete) %*% b + rnorm(n, 0, sigma.eps)
reg.complete <- lm(y.complete ~ X.complete)


# Add missing values
yX.miss <- ampute(cbind(y.complete, X.complete), 0.15,
  patterns = matrix(
    c(
      0, 0, 1,
      0, 1, 0,
      1, 0, 0,
      0, 1, 1,
      1, 0, 1,
      1, 1, 0
    ),
    ncol = 3, byrow = TRUE
  ), freq = c(1, 1, 1, 2, 2, 2) / 9,
  mech = "MCAR", bycases = FALSE
)
y <- yX.miss$amp[, 1] # OUTPUT of the linear regression with NAs
X <- as.matrix(yX.miss$amp[, 2:3]) # INPUT of the linear regression with NAs
```

### 2.2 Estimate parameters with EM algorithm(theoritical part to be done)
```{r}
# Sort rows by missing patterns, then centrals/scales columns of input
# Under normal distribution assumption, we want to estimate the parameters of this normal distribution by EM algorithm
s <- prelim.norm(cbind(y, X))
thetahat <- em.norm(s)
pars <- getparam.norm(s, thetahat)
```
```{r}
# Calculate regression estimates with explicit formulas
b.est <- c(
  pars$mu[1] -
    pars$sigma[1, 2:3] %*% solve(pars$sigma[2:3, 2:3]) %*% pars$mu[2:3],
  pars$sigma[1, 2:3] %*% solve(pars$sigma[2:3, 2:3])
)
esigma.est <- as.vector(sqrt(
  pars$sigma[1, 1] - b.est[2:3] %*% pars$sigma[2:3, 2:3] %*% b.est[2:3]
))
Gram.est <- rbind(rep(0, 3), cbind(rep(0, 2), pars$sigma[2:3, 2:3])) +
  c(1, pars$mu[2:3]) %*% t(c(1, pars$mu[2:3]))
sdb.est <- sqrt(diag(esigma.est^2 * solve(Gram.est * n)))
cat("Estimated regression coefficients:", b.est, "\n")
cat("Their standard deviations:", sdb.est, "\n")
cat("Standard deviation of residuals:", esigma.est, "\n")
```
### 2.3 Estimate parameters with Multiple Imputation algorithm

```{r}
# Using R-package "mice"
mice.est <- mice(cbind(y, X), maxit = 5, m = 20, printFlag = FALSE) # impute
fit <- with(data = mice.est, exp = lm(y ~ V2 + V3)) # fit all m models
mice.lmodel <- mice::pool(fit) # pool the results using Rubin's rules
summary(mice.lmodel)
```
## 3. Logistic regression
### 3.1 Generate data with missing values MCAR
```{r}
# Generate dataset
set.seed(200)
N <- 500 # number of subjects
p <- 5 # number of explanatory variables
mu.star <- 1:p # rep(0,p) # mean of the explanatory variables
sd <- 1:p # rep(1,p) # standard deviations
C <- matrix(c( # correlation matrix
  1, 0.8, 0, 0, 0,
  0.8, 1, 0, 0, 0,
  0, 0, 1, 0.3, 0.6,
  0, 0, 0.3, 1, 0.7,
  0, 0, 0.6, 0.7, 1
), nrow = p)
Sigma.star <- diag(sd) %*% C %*% diag(sd) # covariance matrix
beta.star <- c(1, -1, 1, 0, -1) # coefficients
beta0.star <- 0 # intercept
beta.true <- c(beta0.star, beta.star)
# Design matrix
X.complete <- matrix(rnorm(N * p), nrow = N) %*% chol(Sigma.star) +
  matrix(rep(mu.star, N), nrow = N, byrow = TRUE)
# Reponse vector
p1 <- 1 / (1 + exp(-X.complete %*% beta.star - beta0.star))
y <- as.numeric(runif(N) < p1)
# Generate missingness
set.seed(200)
p.miss <- 0.10
patterns <- runif(N * p) < p.miss # MCAR
X.obs <- X.complete
X.obs[patterns] <- NA
```

### 3.2 Estimate parameters with EM algorithm(theoritical part to be done)
```{r}
library(misaem)
df.obs <- data.frame(y, X.obs)
miss.list <- miss.glm(y ~ ., data = df.obs, print_iter = FALSE, seed = 100)
print(summary(miss.list))
print(miss.list$var.covar)
```

### 3.3 Estimate parameters with Multiple Imputation algorithm

```{r}
# imputation of 20 complete datasets
mi <- mice(data.frame(y, X.obs), m = 20, printFlag = FALSE)
# complete.dataset.1 <- complete(mi, action=1) # completed dataset #1
# fit, family = binomial indicates the logistic regression
fit <- with(data = mi, exp = glm(y ~ X1 + X2 + X3 + X4 + X5, family = binomial))
beta.mi <- mice::pool(fit) # pool the results using Rubin's rules
summary(beta.mi)
# The variance here is calculated by Rubin's rule
```




